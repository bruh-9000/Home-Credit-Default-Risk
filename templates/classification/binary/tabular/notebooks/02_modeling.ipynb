{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f987f91e",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccbeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "import joblib\n",
    "import warnings\n",
    "import shap\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "ROOT = Path().resolve().parent\n",
    "SAVE_DIR = ROOT / \"saved\"\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "label = config[\"general\"][\"label\"]\n",
    "value_mappings = config[\"preprocessing\"][\"value_mappings\"]\n",
    "primary_metric = config[\"general\"][\"primary_metric\"]\n",
    "\n",
    "from utils.utils import (\n",
    "    evaluate_pipeline,\n",
    "    summarize_model_results,\n",
    "    cleaning_pipeline,\n",
    "    preprocessor\n",
    ")\n",
    "\n",
    "X = joblib.load(SAVE_DIR / \"X.pkl\")\n",
    "X_train = joblib.load(SAVE_DIR / \"X_train.pkl\")\n",
    "y_train = joblib.load(SAVE_DIR / \"y_train.pkl\")\n",
    "X_test = joblib.load(SAVE_DIR / \"X_test.pkl\")\n",
    "y_test = joblib.load(SAVE_DIR / \"y_test.pkl\")\n",
    "X_val = joblib.load(SAVE_DIR / \"X_val.pkl\")\n",
    "y_val = joblib.load(SAVE_DIR / \"y_val.pkl\")\n",
    "\n",
    "\n",
    "X_train_cleaned = cleaning_pipeline.fit_transform(X_train)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_train_preprocessed = pd.DataFrame(\n",
    "    preprocessor.fit_transform(X_train_cleaned, y_train),\n",
    "    columns=preprocessor.get_feature_names_out()\n",
    ")\n",
    "\n",
    "\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "# X_train_preprocessed, y_train = rus.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "# from imblearn.combine import SMOTEENN\n",
    "# sampler = SMOTEENN(sampling_strategy=0.5, random_state=42)\n",
    "# X_train_preprocessed, y_train = sampler.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "\n",
    "X_test_cleaned = cleaning_pipeline.transform(X_test)\n",
    "X_test_preprocessed = pd.DataFrame(\n",
    "    preprocessor.transform(X_test_cleaned),\n",
    "    columns=preprocessor.get_feature_names_out()\n",
    ")\n",
    "\n",
    "X_val_cleaned = cleaning_pipeline.transform(X_val)\n",
    "X_val_preprocessed = pd.DataFrame(\n",
    "    preprocessor.transform(X_val_cleaned),\n",
    "    columns=preprocessor.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Bundle into one variable for evaluation\n",
    "data = [X, X_train_preprocessed, y_train, X_test_preprocessed, y_test, X_val_preprocessed, y_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce52941",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7afe1",
   "metadata": {},
   "source": [
    "### DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pipeline, dummy_data = evaluate_pipeline('dummy_classifier', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eef4ef",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline, logistic_data = evaluate_pipeline('logistic_regression', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df153816",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pipeline, forest_data = evaluate_pipeline('random_forest', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854faf2a",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e028d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_pipeline, light_data = evaluate_pipeline('lightgbm', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a79aef",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aead387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precomputed metric results from evaluation\n",
    "model_data = {\n",
    "    'dummy_classifier': dummy_data,\n",
    "    'logistic_regression': logistic_data,\n",
    "    'random_forest': forest_data,\n",
    "    'lightgbm': light_data\n",
    "}\n",
    "\n",
    "fitted_pipelines = {\n",
    "    'dummy_classifier': dummy_pipeline,\n",
    "    'logistic_regression': logistic_pipeline,\n",
    "    'random_forest': forest_pipeline,\n",
    "    'lightgbm': light_pipeline\n",
    "}\n",
    "\n",
    "metrics_to_display = {\n",
    "    'F1': '{:.2%}',\n",
    "    'Accuracy': '{:.2%}',\n",
    "    'Precision': '{:.2%}'\n",
    "}\n",
    "\n",
    "best_model, best_pipeline = summarize_model_results(\n",
    "    model_data,\n",
    "    primary_metric,\n",
    "    metrics_to_display,\n",
    "    fitted_pipelines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on training data\n",
    "y_train_pred = best_pipeline.predict(X_train_preprocessed)\n",
    "\n",
    "# Only get wrong predictions\n",
    "wrong_mask = y_train_pred != y_train\n",
    "wrong_mask = wrong_mask\n",
    "X_train_preprocessed = X_train_preprocessed\n",
    "y_train = y_train\n",
    "wrong_examples = X_train_preprocessed[wrong_mask.values]\n",
    "\n",
    "label_map = {\n",
    "    0: f\"Not {label}\",\n",
    "    1: label.capitalize()\n",
    "}\n",
    "\n",
    "display = wrong_examples.copy()\n",
    "display['Actual'] = y_train[wrong_mask].values\n",
    "display['Predicted'] = y_train_pred[wrong_mask]\n",
    "\n",
    "display['Actual'] = display['Actual'].map(label_map)\n",
    "display['Predicted'] = display['Predicted'].map(label_map)\n",
    "\n",
    "# Show a few wrong examples\n",
    "display.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda5652",
   "metadata": {},
   "source": [
    "#### Model performance metrics\n",
    "Summarize the key evaluation metrics and what they say about the model's overall predictive power. Highlight any strengths or weaknesses revealed by these numbers\n",
    "\n",
    "#### Feature importance analysis\n",
    "Describe which features contribute most to the model's decisions. Include insights from feature importance scores, SHAP values, and more. Explain why certain features might be especially influential\n",
    "\n",
    "#### Overfitting/underfitting and generalization\n",
    "Discuss evidence of overfitting/underfitting, if any. Use training vs. validation scores, learning curves, or cross-validation results to support analysis. Explain how well the model is expected to perform on unseen data\n",
    "\n",
    "#### Comparison to baseline models\n",
    "Compare the model's performance to the basline models, Dummy and Logistic Regression. Highlight improvements and explain why this model is a better choice than those options\n",
    "\n",
    "#### Error analysis\n",
    "Common failure cases (e.g., certain classes, edge cases) and examples of misclassified instances\n",
    "\n",
    "#### Model deployment considerations\n",
    "Inference time and scalability. Will it work for real time predictions?\n",
    "\n",
    "#### Data quality and preprocessing impact\n",
    "Effect of missing data handling and impact of feature engineering\n",
    "\n",
    "#### Summary\n",
    "Brief summary containing the most important points from the above information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ab9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_row_idx = wrong_examples.index[0]\n",
    "\n",
    "actual_value = display.loc[wrong_row_idx, 'Actual']\n",
    "predicted_value = display.loc[wrong_row_idx, 'Predicted']\n",
    "\n",
    "model = best_pipeline.named_steps['model']\n",
    "\n",
    "shap_input = X_train_preprocessed.loc[[wrong_row_idx]]\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(shap_input)\n",
    "shap_values_instance = shap_values[1] \n",
    "shap_values_instance = shap.Explanation(\n",
    "    values=shap_values_instance[0].flatten(),  # Flatten the values to 1D to match the shape\n",
    "    base_values=explainer.expected_value[1],  # Base value for the positive class\n",
    "    data=shap_input.values.flatten(),  # Flatten the input values to match the expected shape\n",
    "    feature_names=X_train_preprocessed.columns.tolist()\n",
    ")\n",
    "\n",
    "print(f\"\\nSHAP explanation: (Pred: {predicted_value}, Actual: {actual_value})\")\n",
    "\n",
    "shap.plots.waterfall(shap_values_instance,show=False)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Add custom labels on the left and right side of the plot\n",
    "# Left label\n",
    "ax.annotate(f\"Not {label}\", xy=(0, -.1), xycoords='axes fraction', \n",
    "            horizontalalignment='left', verticalalignment='bottom', fontsize=12, color='black')\n",
    "\n",
    "# Right label\n",
    "ax.annotate(label.capitalize(), xy=(1, -.1), xycoords='axes fraction', \n",
    "            horizontalalignment='right', verticalalignment='bottom', fontsize=12, color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630abeb",
   "metadata": {},
   "source": [
    "Analyze what went wrong and why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5cdfd",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "#### Key insights\n",
    "\n",
    "#### Limitations and possible improvements\n",
    "\n",
    "#### Business implications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
