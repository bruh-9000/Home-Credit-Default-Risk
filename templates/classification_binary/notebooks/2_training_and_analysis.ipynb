{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f987f91e",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccbeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import warnings\n",
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# Add parent directory to sys.path for local imports\n",
    "ROOT = Path().resolve().parent\n",
    "SAVE_DIR = ROOT / \"saved\"\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "# Load config.yaml\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Access primary metric\n",
    "primary_metric = config[\"general\"][\"primary_metric\"]\n",
    "\n",
    "# Import relevant utility functions\n",
    "from utils.utils import (\n",
    "    evaluate_pipeline,\n",
    "    summarize_model_results\n",
    ")\n",
    "\n",
    "# Load data from disk\n",
    "X = joblib.load(SAVE_DIR / \"X.pkl\")\n",
    "X_train = joblib.load(SAVE_DIR / \"X_train.pkl\")\n",
    "y_train = joblib.load(SAVE_DIR / \"y_train.pkl\")\n",
    "X_test = joblib.load(SAVE_DIR / \"X_test.pkl\")\n",
    "y_test = joblib.load(SAVE_DIR / \"y_test.pkl\")\n",
    "\n",
    "# Bundle if needed\n",
    "data = [X, X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce52941",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7afe1",
   "metadata": {},
   "source": [
    "### DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pipeline, dummy_data = evaluate_pipeline('dummy_classifier', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eef4ef",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline, logistic_data = evaluate_pipeline('logistic_regression', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df153816",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pipeline, forest_data = evaluate_pipeline('random_forest', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854faf2a",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e028d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_pipeline, light_data = evaluate_pipeline('lightgbm', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a79aef",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aead387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precomputed metric results from evaluation\n",
    "model_data = {\n",
    "    'dummy_classifier': dummy_data,\n",
    "    'logistic_regression': logistic_data,\n",
    "    'random_forest': forest_data,\n",
    "    'lightgbm': light_data\n",
    "}\n",
    "\n",
    "fitted_pipelines = {\n",
    "    'dummy_classifier': dummy_pipeline,\n",
    "    'logistic_regression': logistic_pipeline,\n",
    "    'random_forest': forest_pipeline,\n",
    "    'lightgbm': light_pipeline\n",
    "}\n",
    "\n",
    "metrics_to_display = {\n",
    "    'F1': '{:.2%}',\n",
    "    'Accuracy': '{:.2%}',\n",
    "    'Precision': '{:.2%}'\n",
    "}\n",
    "\n",
    "best_model, best_pipeline = summarize_model_results(\n",
    "    model_data,\n",
    "    primary_metric,\n",
    "    metrics_to_display,\n",
    "    fitted_pipelines\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda5652",
   "metadata": {},
   "source": [
    "#### Model performance metrics\n",
    "Summarize the key evaluation metrics and what they say about the model's overall predictive power. Highlight any strengths or weaknesses revealed by these numbers\n",
    "\n",
    "#### Feature importance analysis\n",
    "Describe which features contribute most to the model's decisions. Include insights from feature importance scores, SHAP values, and more. Explain why certain features might be especially influential\n",
    "\n",
    "#### Overfitting/underfitting and generalization\n",
    "Discuss evidence of overfitting/underfitting, if any. Use training vs. validation scores, learning curves, or cross-validation results to support analysis. Explain how well the model is expected to perform on unseen data\n",
    "\n",
    "#### Comparison to baseline models\n",
    "Compare the model's performance to the basline models, Dummy and Logistic Regression. Highlight improvements and explain why this model is a better choice than those options\n",
    "\n",
    "#### Error analysis\n",
    "Common failure cases (e.g., certain classes, edge cases) and examples of misclassified instances\n",
    "\n",
    "#### Model deployment considerations\n",
    "Inference time and scalability. Will it work for real time predictions?\n",
    "\n",
    "#### Data quality and preprocessing impact\n",
    "Effect of missing data handling and impact of feature engineering\n",
    "\n",
    "#### Summary\n",
    "Brief summary containing the most important points from the above information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ab9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "# Find first incorrect prediction\n",
    "if len(np.where(y_pred != y_test)[0]) > 0:\n",
    "    wrong_index = np.where(y_pred != y_test)[0][0]\n",
    "\n",
    "    # Transform the test set using the fitted preprocessor\n",
    "    X_transformed = best_pipeline.named_steps['preprocessing'].transform(X_test)\n",
    "    features = best_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "    X_df = pd.DataFrame(X_transformed, columns=features)\n",
    "\n",
    "    try:\n",
    "        # Generate SHAP explainer\n",
    "        explainer = shap.Explainer(best_pipeline.named_steps['model'], X_df, feature_names=features)\n",
    "        shap_values = explainer(X_df.iloc[[wrong_index]])\n",
    "\n",
    "        # Plot SHAP waterfall\n",
    "        shap.plots.waterfall(shap_values[wrong_index])\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f'SHAP could not generate explanation: {e}')\n",
    "else:\n",
    "    print(f'No incorrect predictions for {best_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630abeb",
   "metadata": {},
   "source": [
    "Analyze what went wrong and why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5cdfd",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "#### Key insights\n",
    "\n",
    "#### Limitations and possible improvements\n",
    "\n",
    "#### Business implications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
